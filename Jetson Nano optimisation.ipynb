{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN1TAy+TdLN0jZoqp0eftmQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iv7ftyxyiCvp","executionInfo":{"status":"ok","timestamp":1762373313182,"user_tz":-60,"elapsed":1674976,"user":{"displayName":"Houssem Eddine Lahmar","userId":"04893437730640872386"}},"outputId":"5b9f1123-ccdd-4b35-c591-dc7cceb2755a"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","STEP 1: Mounting Google Drive\n","============================================================\n","Mounted at /content/drive\n","\n","============================================================\n","STEP 2: Installing Dependencies\n","============================================================\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\n","============================================================\n","STEP 3: Checking GPU\n","============================================================\n","CUDA Available: True\n","GPU Name: Tesla T4\n","GPU Memory: 15.83 GB\n","\n","============================================================\n","STEP 4: Model Location Setup\n","============================================================\n","‚úÖ Model found: /content/drive/MyDrive/yolov8m.pt\n","   File size: 49.70 MB\n","\n","============================================================\n","STEP 5: Creating Output Directory\n","============================================================\n","‚úÖ Output directory: /content/drive/MyDrive/optimized_models\n","\n","============================================================\n","STEP 6: Loading YOLOv8 Model\n","============================================================\n","Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","‚úÖ Model loaded successfully\n","   Model type: DetectionModel\n","\n","============================================================\n","STEP 7: Exporting Optimized Models\n","============================================================\n","\n","[1/4] Exporting to ONNX...\n","   What: Universal neural network format\n","   Optimization: Graph simplification\n","   Quantization: None (FP32)\n","   Best for: Portability, intermediate format\n","Ultralytics 8.3.225 üöÄ Python-3.12.12 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n","üí° ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n","YOLOv8m summary (fused): 92 layers, 25,886,080 parameters, 0 gradients, 78.9 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/MyDrive/yolov8m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (49.7 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnxslim>=0.1.71', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 3.7s\n","WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 22...\n","\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.73...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 9.8s, saved as '/content/drive/MyDrive/yolov8m.onnx' (99.0 MB)\n","\n","Export complete (14.1s)\n","Results saved to \u001b[1m/content/drive/MyDrive\u001b[0m\n","Predict:         yolo predict task=detect model=/content/drive/MyDrive/yolov8m.onnx imgsz=640  \n","Validate:        yolo val task=detect model=/content/drive/MyDrive/yolov8m.onnx imgsz=640 data=coco.yaml  \n","Visualize:       https://netron.app\n","   ‚úÖ ONNX export successful: /content/drive/MyDrive/yolov8m.onnx\n","   ‚úÖ Saved to Drive: /content/drive/MyDrive/optimized_models/yolov8m.onnx\n","\n","[2/4] Exporting to TensorRT FP16...\n","   What: NVIDIA optimized inference engine\n","   Optimization: Layer fusion, kernel auto-tuning, memory optimization\n","   Quantization: FP16 (16-bit floating point)\n","   Speed: ~2-3x faster than PyTorch\n","   Accuracy: ~99% of original (minimal loss)\n","   Best for: Jetson Nano (RECOMMENDED)\n","Ultralytics 8.3.225 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","YOLOv8m summary (fused): 92 layers, 25,886,080 parameters, 0 gradients, 78.9 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/MyDrive/yolov8m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (49.7 MB)\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 20...\n","\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.73...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 2.4s, saved as '/content/drive/MyDrive/yolov8m.onnx' (99.0 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['tensorrt-cu12>7.0.0,!=10.1.0'] not found, attempting AutoUpdate...\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 119.3s\n","WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.3.9...\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as /content/drive/MyDrive/yolov8m.engine\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 413.6s, saved as '/content/drive/MyDrive/yolov8m.engine' (52.7 MB)\n","\n","Export complete (415.4s)\n","Results saved to \u001b[1m/content/drive/MyDrive\u001b[0m\n","Predict:         yolo predict task=detect model=/content/drive/MyDrive/yolov8m.engine imgsz=640 half \n","Validate:        yolo val task=detect model=/content/drive/MyDrive/yolov8m.engine imgsz=640 data=coco.yaml half \n","Visualize:       https://netron.app\n","   ‚úÖ TensorRT FP16 export successful: /content/drive/MyDrive/yolov8m.engine\n","   ‚úÖ Saved to Drive: /content/drive/MyDrive/optimized_models/yolov8m_fp16.engine\n","\n","[3/4] Exporting to TensorRT INT8...\n","   What: NVIDIA optimized with integer quantization\n","   Optimization: FP16 optimizations + INT8 quantization\n","   Quantization: INT8 (8-bit integers)\n","   Speed: ~3-4x faster than PyTorch\n","   Accuracy: ~97-98% of original (1-3% loss)\n","   Requires: Calibration dataset\n","   Best for: Maximum performance (if accuracy acceptable)\n","   Downloading calibration dataset...\n","   Starting INT8 calibration (this may take 5-10 minutes)...\n","Ultralytics 8.3.225 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","YOLOv8m summary (fused): 92 layers, 25,886,080 parameters, 0 gradients, 78.9 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/MyDrive/yolov8m.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (49.7 MB)\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 20...\n","\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.73...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 2.6s, saved as '/content/drive/MyDrive/yolov8m.onnx' (99.0 MB)\n","\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.3.9...\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m collecting INT8 calibration images from 'data=coco128.yaml'\n","\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 25.1MB/s 0.0s\n","Fast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 357.1¬±257.1 MB/s, size: 66.5 KB)\n","\u001b[KScanning /content/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 128/128 1.1Kit/s 0.1s\n","New cache created: /content/coco128/labels/train2017.cache\n","WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mTensorRT:\u001b[0m >300 images recommended for INT8 calibration, found 128 images.\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m building INT8 engine as /content/drive/MyDrive/yolov8m.engine\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 357.1s, saved as '/content/drive/MyDrive/yolov8m.engine' (29.1 MB)\n","\n","Export complete (357.4s)\n","Results saved to \u001b[1m/content/drive/MyDrive\u001b[0m\n","Predict:         yolo predict task=detect model=/content/drive/MyDrive/yolov8m.engine imgsz=640 int8 \n","Validate:        yolo val task=detect model=/content/drive/MyDrive/yolov8m.engine imgsz=640 data=coco.yaml int8 \n","Visualize:       https://netron.app\n","   ‚úÖ TensorRT INT8 export successful: /content/drive/MyDrive/yolov8m.engine\n","   ‚úÖ Saved to Drive: /content/drive/MyDrive/optimized_models/yolov8m_int8.engine\n","\n","[4/4] Exporting TensorRT FP16 with 320x320 input...\n","   What: Same as #2 but with smaller input resolution\n","   Optimization: Same FP16 optimizations\n","   Quantization: FP16\n","   Speed: ~2x faster than 640x640 (total ~4-6x vs PyTorch)\n","   Trade-off: Slightly less accurate for small objects\n","   Best for: Real-time applications on Jetson\n","Ultralytics 8.3.225 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n","YOLOv8m summary (fused): 92 layers, 25,886,080 parameters, 0 gradients, 78.9 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/MyDrive/yolov8m.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 84, 2100) (49.7 MB)\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 20...\n","\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.73...\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 2.6s, saved as '/content/drive/MyDrive/yolov8m.onnx' (98.9 MB)\n","\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.13.3.9...\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 320, 320) DataType.FLOAT\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 2100) DataType.FLOAT\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as /content/drive/MyDrive/yolov8m.engine\n","\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 315.9s, saved as '/content/drive/MyDrive/yolov8m.engine' (53.3 MB)\n","\n","Export complete (316.2s)\n","Results saved to \u001b[1m/content/drive/MyDrive\u001b[0m\n","Predict:         yolo predict task=detect model=/content/drive/MyDrive/yolov8m.engine imgsz=320 half \n","Validate:        yolo val task=detect model=/content/drive/MyDrive/yolov8m.engine imgsz=320 data=coco.yaml half \n","Visualize:       https://netron.app\n","   ‚úÖ TensorRT FP16 320x320 export successful: /content/drive/MyDrive/yolov8m.engine\n","   ‚úÖ Saved to Drive: /content/drive/MyDrive/optimized_models/yolov8m_fp16_320.engine\n","\n","============================================================\n","STEP 8: Export Summary\n","============================================================\n","\n","üì¶ Exported Models:\n","------------------------------------------------------------\n","‚úÖ yolov8m.onnx                             ( 99.00 MB)\n","‚úÖ yolov8m_fp16.engine                      ( 52.66 MB)\n","‚úÖ yolov8m_int8.engine                      ( 29.13 MB)\n","‚úÖ yolov8m_fp16_320.engine                  ( 53.28 MB)\n","------------------------------------------------------------\n","Total models exported: 4\n","\n","============================================================\n","STEP 9: Next Steps for Jetson Nano\n","============================================================\n","\n","üìã OPTIMIZATIONS APPLIED:\n","\n","1. ONNX Export:\n","   - Graph simplification\n","   - No quantization (FP32)\n","   - Portable format\n","\n","2. TensorRT FP16 (640x640): ‚≠ê RECOMMENDED\n","   - Quantization: FP32 ‚Üí FP16 (16-bit)\n","   - Layer fusion and kernel optimization\n","   - ~2-3x faster, <1% accuracy loss\n","   - File: *_fp16.engine\n","\n","3. TensorRT INT8 (640x640): (If successful)\n","   - Quantization: FP32 ‚Üí INT8 (8-bit)\n","   - Calibrated on COCO128 dataset\n","   - ~3-4x faster, 1-3% accuracy loss\n","   - File: *_int8.engine\n","\n","4. TensorRT FP16 (320x320): ‚≠ê BEST FOR REALTIME\n","   - Same as #2 but smaller input\n","   - ~4-6x faster than original\n","   - Best balance for Jetson Nano\n","   - File: *_fp16_320.engine\n","\n","üì• TRANSFER TO JETSON:\n","\n","1. Download files from Google Drive:\n","\n","   /content/drive/MyDrive/optimized_models\n","\n","2. Transfer via SCP (from your PC):\n","   scp optimized_models/*.onnx jetson@<jetson-ip>:~/models/\n","   scp optimized_models/*.engine jetson@<jetson-ip>:~/models/\n","\n","3. Or use USB drive / direct download from Drive\n","\n","‚ö†Ô∏è IMPORTANT NOTES:\n","\n","- .engine files are GPU-specific!\n","- Engine files from Colab may NOT work on Jetson\n","- SOLUTION: Transfer .onnx file and convert on Jetson:\n","  \n","  On Jetson Nano:\n","  from ultralytics import YOLO\n","  model = YOLO('yolov8m.onnx')\n","  model.export(format='engine', half=True)\n","\n","üöÄ RECOMMENDED WORKFLOW:\n","\n","1. Transfer yolov8m.onnx to Jetson (most portable)\n","2. On Jetson, convert ONNX ‚Üí TensorRT FP16:\n","   python3 -c \"from ultralytics import YOLO; YOLO('yolov8m.onnx').export(format='engine', half=True, imgsz=320)\"\n","3. Use the generated .engine file for inference\n","\n","üí° Expected Performance on Jetson Nano:\n","   - Original .pt (FP32, 640x640): 1-2 FPS\n","   - TensorRT FP16 (640x640): 3-5 FPS\n","   - TensorRT FP16 (320x320): 8-12 FPS ‚≠ê BEST\n","   - TensorRT INT8 (320x320): 10-15 FPS (if calibrated well)\n","\n","============================================================\n","‚úÖ OPTIMIZATION COMPLETE!\n","============================================================\n","\n","üìÇ All optimized models saved to:\n","   /content/drive/MyDrive/optimized_models\n","\n","You can download them from your Google Drive now!\n"]}],"source":["# ============================================\n","# YOLOv8 Model Optimization for Jetson Nano\n","# Run this in Google Colab\n","# ============================================\n","\n","# STEP 1: Mount Google Drive\n","print(\"=\"*60)\n","print(\"STEP 1: Mounting Google Drive\")\n","print(\"=\"*60)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# STEP 2: Install Required Libraries\n","print(\"\\n\" + \"=\"*60)\n","print(\"STEP 2: Installing Dependencies\")\n","print(\"=\"*60)\n","\n","!pip install ultralytics -q\n","!pip install onnx onnxsim -q\n","\n","# STEP 3: Check GPU Availability\n","print(\"\\n\" + \"=\"*60)\n","print(\"STEP 3: Checking GPU\")\n","print(\"=\"*60)\n","\n","import torch\n","print(f\"CUDA Available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n","    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","else:\n","    print(\"‚ö†Ô∏è WARNING: No GPU detected. Optimization will be slower.\")\n","\n","# STEP 4: Locate Your Model\n","print(\"\\n\" + \"=\"*60)\n","print(\"STEP 4: Model Location Setup\")\n","print(\"=\"*60)\n","\n","# UPDATE THIS PATH to where your yolov8m.pt is located in Google Drive\n","MODEL_PATH = '/content/drive/MyDrive/yolov8m.pt'  # <-- CHANGE THIS PATH\n","\n","# Check if model exists\n","import os\n","if os.path.exists(MODEL_PATH):\n","    print(f\"‚úÖ Model found: {MODEL_PATH}\")\n","    file_size = os.path.getsize(MODEL_PATH) / (1024*1024)\n","    print(f\"   File size: {file_size:.2f} MB\")\n","else:\n","    print(f\"‚ùå Model NOT found at: {MODEL_PATH}\")\n","    print(\"\\nPlease update MODEL_PATH variable above to match your file location.\")\n","    print(\"Example paths:\")\n","    print(\"  - '/content/drive/MyDrive/yolov8m.pt'\")\n","    print(\"  - '/content/drive/MyDrive/models/yolov8m.pt'\")\n","    raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n","\n","# STEP 5: Create Output Directory\n","print(\"\\n\" + \"=\"*60)\n","print(\"STEP 5: Creating Output Directory\")\n","print(\"=\"*60)\n","\n","OUTPUT_DIR = '/content/drive/MyDrive/optimized_models'\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","print(f\"‚úÖ Output directory: {OUTPUT_DIR}\")\n","\n","# STEP 6: Load Model\n","print(\"\\n\" + \"=\"*60)\n","print(\"STEP 6: Loading YOLOv8 Model\")\n","print(\"=\"*60)\n","\n","from ultralytics import YOLO\n","\n","model = YOLO(MODEL_PATH)\n","print(f\"‚úÖ Model loaded successfully\")\n","print(f\"   Model type: {model.model.__class__.__name__}\")\n","\n","# STEP 7: Export to Different Formats\n","print(\"\\n\" + \"=\"*60)\n","print(\"STEP 7: Exporting Optimized Models\")\n","print(\"=\"*60)\n","\n","# ---------------------------------------------\n","# Export 1: ONNX (Universal format)\n","# ---------------------------------------------\n","print(\"\\n[1/4] Exporting to ONNX...\")\n","print(\"   What: Universal neural network format\")\n","print(\"   Optimization: Graph simplification\")\n","print(\"   Quantization: None (FP32)\")\n","print(\"   Best for: Portability, intermediate format\")\n","\n","try:\n","    onnx_path = model.export(\n","        format='onnx',\n","        simplify=True,  # Simplifies the computation graph\n","        dynamic=False,   # Fixed input size for better optimization\n","        imgsz=640        # Input size\n","    )\n","    print(f\"   ‚úÖ ONNX export successful: {onnx_path}\")\n","\n","    # Copy to Google Drive\n","    import shutil\n","    onnx_filename = os.path.basename(onnx_path)\n","    drive_onnx = os.path.join(OUTPUT_DIR, onnx_filename)\n","    shutil.copy(onnx_path, drive_onnx)\n","    print(f\"   ‚úÖ Saved to Drive: {drive_onnx}\")\n","except Exception as e:\n","    print(f\"   ‚ùå ONNX export failed: {e}\")\n","\n","# ---------------------------------------------\n","# Export 2: TensorRT FP16 (RECOMMENDED)\n","# ---------------------------------------------\n","print(\"\\n[2/4] Exporting to TensorRT FP16...\")\n","print(\"   What: NVIDIA optimized inference engine\")\n","print(\"   Optimization: Layer fusion, kernel auto-tuning, memory optimization\")\n","print(\"   Quantization: FP16 (16-bit floating point)\")\n","print(\"   Speed: ~2-3x faster than PyTorch\")\n","print(\"   Accuracy: ~99% of original (minimal loss)\")\n","print(\"   Best for: Jetson Nano (RECOMMENDED)\")\n","\n","try:\n","    trt_fp16_path = model.export(\n","        format='engine',\n","        half=True,       # Enable FP16 quantization\n","        device=0,        # Use GPU\n","        workspace=4,     # Max workspace size in GB\n","        imgsz=640\n","    )\n","    print(f\"   ‚úÖ TensorRT FP16 export successful: {trt_fp16_path}\")\n","\n","    # Copy to Google Drive\n","    trt_filename = os.path.basename(trt_fp16_path)\n","    drive_trt = os.path.join(OUTPUT_DIR, trt_filename.replace('.engine', '_fp16.engine'))\n","    shutil.copy(trt_fp16_path, drive_trt)\n","    print(f\"   ‚úÖ Saved to Drive: {drive_trt}\")\n","except Exception as e:\n","    print(f\"   ‚ùå TensorRT FP16 export failed: {e}\")\n","    print(\"   Note: TensorRT exports are GPU-specific. Re-export on Jetson for best compatibility.\")\n","\n","# ---------------------------------------------\n","# Export 3: TensorRT INT8 (ADVANCED)\n","# ---------------------------------------------\n","print(\"\\n[3/4] Exporting to TensorRT INT8...\")\n","print(\"   What: NVIDIA optimized with integer quantization\")\n","print(\"   Optimization: FP16 optimizations + INT8 quantization\")\n","print(\"   Quantization: INT8 (8-bit integers)\")\n","print(\"   Speed: ~3-4x faster than PyTorch\")\n","print(\"   Accuracy: ~97-98% of original (1-3% loss)\")\n","print(\"   Requires: Calibration dataset\")\n","print(\"   Best for: Maximum performance (if accuracy acceptable)\")\n","\n","try:\n","    # Download calibration dataset (COCO128 - small subset)\n","    print(\"   Downloading calibration dataset...\")\n","    !wget -q https://ultralytics.com/assets/coco128.zip\n","    !unzip -q coco128.zip\n","\n","    # Create data.yaml for calibration\n","    with open('coco128.yaml', 'w') as f:\n","        f.write(\"\"\"\n","path: coco128\n","train: images/train2017\n","val: images/train2017\n","\n","nc: 80\n","names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n","        'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n","        'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n","        'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n","        'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n","        'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n","        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n","        'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n","        'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n","        'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n","\"\"\")\n","\n","    print(\"   Starting INT8 calibration (this may take 5-10 minutes)...\")\n","    trt_int8_path = model.export(\n","        format='engine',\n","        int8=True,           # Enable INT8 quantization\n","        data='coco128.yaml', # Calibration dataset\n","        device=0,\n","        workspace=4,\n","        imgsz=640,\n","        batch=1\n","    )\n","    print(f\"   ‚úÖ TensorRT INT8 export successful: {trt_int8_path}\")\n","\n","    # Copy to Google Drive\n","    trt_int8_filename = os.path.basename(trt_int8_path)\n","    drive_trt_int8 = os.path.join(OUTPUT_DIR, trt_int8_filename.replace('.engine', '_int8.engine'))\n","    shutil.copy(trt_int8_path, drive_trt_int8)\n","    print(f\"   ‚úÖ Saved to Drive: {drive_trt_int8}\")\n","except Exception as e:\n","    print(f\"   ‚ùå TensorRT INT8 export failed: {e}\")\n","    print(\"   Tip: INT8 export requires GPU and may fail on some Colab instances\")\n","\n","# ---------------------------------------------\n","# Export 4: Smaller Input Sizes (320x320)\n","# ---------------------------------------------\n","print(\"\\n[4/4] Exporting TensorRT FP16 with 320x320 input...\")\n","print(\"   What: Same as #2 but with smaller input resolution\")\n","print(\"   Optimization: Same FP16 optimizations\")\n","print(\"   Quantization: FP16\")\n","print(\"   Speed: ~2x faster than 640x640 (total ~4-6x vs PyTorch)\")\n","print(\"   Trade-off: Slightly less accurate for small objects\")\n","print(\"   Best for: Real-time applications on Jetson\")\n","\n","try:\n","    # Reload model to reset export settings\n","    model = YOLO(MODEL_PATH)\n","\n","    trt_320_path = model.export(\n","        format='engine',\n","        half=True,\n","        device=0,\n","        workspace=4,\n","        imgsz=320  # Smaller input size\n","    )\n","    print(f\"   ‚úÖ TensorRT FP16 320x320 export successful: {trt_320_path}\")\n","\n","    # Copy to Google Drive\n","    trt_320_filename = os.path.basename(trt_320_path)\n","    drive_trt_320 = os.path.join(OUTPUT_DIR, trt_320_filename.replace('.engine', '_fp16_320.engine'))\n","    shutil.copy(trt_320_path, drive_trt_320)\n","    print(f\"   ‚úÖ Saved to Drive: {drive_trt_320}\")\n","except Exception as e:\n","    print(f\"   ‚ùå TensorRT 320x320 export failed: {e}\")\n","\n","# STEP 8: Summary\n","print(\"\\n\" + \"=\"*60)\n","print(\"STEP 8: Export Summary\")\n","print(\"=\"*60)\n","\n","print(\"\\nüì¶ Exported Models:\")\n","print(\"-\" * 60)\n","\n","exported_files = []\n","for file in os.listdir(OUTPUT_DIR):\n","    if file.startswith('yolov8'):\n","        file_path = os.path.join(OUTPUT_DIR, file)\n","        file_size = os.path.getsize(file_path) / (1024*1024)\n","        exported_files.append((file, file_size))\n","        print(f\"‚úÖ {file:40s} ({file_size:6.2f} MB)\")\n","\n","if not exported_files:\n","    print(\"‚ùå No models were exported successfully\")\n","else:\n","    print(\"-\" * 60)\n","    print(f\"Total models exported: {len(exported_files)}\")\n","\n","# STEP 9: What to Do Next\n","print(\"\\n\" + \"=\"*60)\n","print(\"STEP 9: Next Steps for Jetson Nano\")\n","print(\"=\"*60)\n","\n","print(\"\"\"\n","üìã OPTIMIZATIONS APPLIED:\n","\n","1. ONNX Export:\n","   - Graph simplification\n","   - No quantization (FP32)\n","   - Portable format\n","\n","2. TensorRT FP16 (640x640): ‚≠ê RECOMMENDED\n","   - Quantization: FP32 ‚Üí FP16 (16-bit)\n","   - Layer fusion and kernel optimization\n","   - ~2-3x faster, <1% accuracy loss\n","   - File: *_fp16.engine\n","\n","3. TensorRT INT8 (640x640): (If successful)\n","   - Quantization: FP32 ‚Üí INT8 (8-bit)\n","   - Calibrated on COCO128 dataset\n","   - ~3-4x faster, 1-3% accuracy loss\n","   - File: *_int8.engine\n","\n","4. TensorRT FP16 (320x320): ‚≠ê BEST FOR REALTIME\n","   - Same as #2 but smaller input\n","   - ~4-6x faster than original\n","   - Best balance for Jetson Nano\n","   - File: *_fp16_320.engine\n","\n","üì• TRANSFER TO JETSON:\n","\n","1. Download files from Google Drive:\n","\"\"\")\n","print(f\"   {OUTPUT_DIR}\")\n","\n","print(\"\"\"\n","2. Transfer via SCP (from your PC):\n","   scp optimized_models/*.onnx jetson@<jetson-ip>:~/models/\n","   scp optimized_models/*.engine jetson@<jetson-ip>:~/models/\n","\n","3. Or use USB drive / direct download from Drive\n","\n","‚ö†Ô∏è IMPORTANT NOTES:\n","\n","- .engine files are GPU-specific!\n","- Engine files from Colab may NOT work on Jetson\n","- SOLUTION: Transfer .onnx file and convert on Jetson:\n","\n","  On Jetson Nano:\n","  from ultralytics import YOLO\n","  model = YOLO('yolov8m.onnx')\n","  model.export(format='engine', half=True)\n","\n","üöÄ RECOMMENDED WORKFLOW:\n","\n","1. Transfer yolov8m.onnx to Jetson (most portable)\n","2. On Jetson, convert ONNX ‚Üí TensorRT FP16:\n","   python3 -c \"from ultralytics import YOLO; YOLO('yolov8m.onnx').export(format='engine', half=True, imgsz=320)\"\n","3. Use the generated .engine file for inference\n","\n","üí° Expected Performance on Jetson Nano:\n","   - Original .pt (FP32, 640x640): 1-2 FPS\n","   - TensorRT FP16 (640x640): 3-5 FPS\n","   - TensorRT FP16 (320x320): 8-12 FPS ‚≠ê BEST\n","   - TensorRT INT8 (320x320): 10-15 FPS (if calibrated well)\n","\"\"\")\n","\n","print(\"=\"*60)\n","print(\"‚úÖ OPTIMIZATION COMPLETE!\")\n","print(\"=\"*60)\n","\n","# Display file locations\n","print(f\"\\nüìÇ All optimized models saved to:\")\n","print(f\"   {OUTPUT_DIR}\")\n","print(\"\\nYou can download them from your Google Drive now!\")"]}]}